{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashankAlluri28/INFO-5731Computational-Methods/blob/main/Alluri_Shashank_In_class_Exercise_2_(1)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBKvD6O_TY6e"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "outputs": [],
      "source": [
        "# write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckV5I_dat2km"
      },
      "source": [
        "What impact does weather variability have on retail customers' purchasing decisions?\n",
        "\n",
        "Data Required:\n",
        "\n",
        "->Daily weather information for one or more regions, including temperature, precipitation, humidity, wind speed, etc.\n",
        "->Retail stores in the same region provide daily sales data.\n",
        "->Information on the region's demographics to account for any socioeconomic influences.\n",
        "->At least a year, collect daily weather data to identify long-term trends and seasonal variations.\n",
        "->For a minimum of a year, track daily sales data to track shifts in consumer behaviour over time.\n",
        "\n",
        "\n",
        "Process of Gathering and Preserving Data:\n",
        "\n",
        "Weather Data:\n",
        "a. Determine trustworthy sources, such as commercial weather data providers, government meteorological agencies, or weather APIs.\n",
        "b. After selecting the region or regions of interest, download daily weather data for the selected time frame, making sure the data format is consistent.\n",
        "c. Store the meteorological data in an organised file type, like CSV or Excel, with columns denoting various meteorological parameters and each row representing a single day.\n",
        "\n",
        "Sales Information:\n",
        " a. Work with retail establishments in the selected region(s) to gain access to their daily sales logs. Verify that data security and confidentiality procedures are followed.\n",
        "b. Gather daily sales data, such as revenue, product categories, sales volume, and other relevant information, for the designated time frame.\n",
        "c. Arrange the sales data into a structured format akin to that of the meteorological data, with columns designating pertinent sales metrics and each row denoting a single day.\n",
        "\n",
        "Demographic Information:\n",
        "a. Acquire demographic information from reliable sources, such as market research companies, statistical agencies, or government census databases.\n",
        "b. Choose pertinent demographic factors for the study, such as age distribution, income levels, and population density.\n",
        "\n",
        "Data Management and Storage:\n",
        "a. Establish a safe, orderly data repository to house the gathered datasets.\n",
        "b. To avoid data loss or corruption, periodically backup your data.\n",
        "c. Verify that ethical standards and data privacy laws are followed at every stage of the data handling procedure.\n",
        "\n",
        "Data Preprocessing and Data Cleaning:\n",
        "a. Clean the datasets to find and fix any errors, missing values, or outliers.\n",
        "b. For consistency, standardise the units of measurement and formats used in various datasets.\n",
        "c. If required, normalise the data to eliminate biases and confounding variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RqrlwdTfvl"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def get_date_ranges(start_date, end_date):\n",
        "    return pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "# Generate synthetic weather data\n",
        "def generate_weather_data(start_date, end_date, region):\n",
        "    dates = get_date_ranges(start_date, end_date)\n",
        "    weather_data = {\n",
        "        'Date': dates,\n",
        "        'Temperature (C)': np.random.randint(-10, 40, size=len(dates)),\n",
        "        'Precipitation (mm)': np.random.uniform(0, 20, size=len(dates)),\n",
        "        'Humidity (%)': np.random.randint(30, 90, size=len(dates)),\n",
        "        'Wind Speed (km/h)': np.random.randint(0, 30, size=len(dates))\n",
        "    }\n",
        "    df_weather = pd.DataFrame(weather_data)\n",
        "    df_weather['Region'] = region\n",
        "    return df_weather\n",
        "\n",
        "# Generate synthetic sales data\n",
        "def generate_sales_data(start_date, end_date, region):\n",
        "    dates = get_date_ranges(start_date, end_date)\n",
        "    sales_data = {\n",
        "        'Date': dates,\n",
        "        'Total Sales': np.random.randint(1000, 10000, size=len(dates)),\n",
        "        'Product Category': [random.choice(['Electronics', 'Clothing', 'Groceries']) for _ in range(len(dates))]\n",
        "    }\n",
        "    df_sales = pd.DataFrame(sales_data)\n",
        "    df_sales['Region'] = region\n",
        "    return df_sales\n",
        "\n",
        "# Generate synthetic demographic data\n",
        "def generate_demographic_data(regions):\n",
        "    demographics = {\n",
        "        'Region': regions,\n",
        "        'Population': [random.randint(50000, 1000000) for _ in range(len(regions))],\n",
        "        'Average Income': [random.randint(20000, 80000) for _ in range(len(regions))]\n",
        "    }\n",
        "    df_demographics = pd.DataFrame(demographics)\n",
        "    return df_demographics\n",
        "\n",
        "# Define regions\n",
        "regions = ['Region A', 'Region B']\n",
        "\n",
        "# Define time frame\n",
        "start_date, end_date = datetime(2023, 1, 1), datetime(2023, 12, 31)\n",
        "\n",
        "# Generate weather data for each region\n",
        "weather_data = pd.concat([generate_weather_data(start_date, end_date, region) for region in regions], ignore_index=True)\n",
        "\n",
        "# Generate sales data for each region\n",
        "sales_data = pd.concat([generate_sales_data(start_date, end_date, region) for region in regions], ignore_index=True)\n",
        "\n",
        "# Generate demographic data\n",
        "demographic_data = generate_demographic_data(regions)\n",
        "\n",
        "# Merge dataframes\n",
        "df = pd.merge(weather_data, sales_data, on=['Date', 'Region'])\n",
        "df = pd.merge(df, demographic_data, on='Region')\n",
        "\n",
        "# Shuffle dataframe\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save dataset to CSV\n",
        "df.to_csv('retail_purchasing_decision_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaGLbSHHB8Ej",
        "outputId": "4e6394ad-aa8e-449a-b21e-8e5650108230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.semanticscholar.org/search?q=XYZ&sort=relevance\n",
            "Status code: 200\n",
            "Scraped 0 articles.\n",
            "No articles were scraped. Please check if the scraping process encountered any issues.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_semantic_scholar(keyword, num_articles):\n",
        "\n",
        "    base_url = \"https://www.semanticscholar.org\"\n",
        "    url = f\"{base_url}/search?q={keyword}&sort=relevance\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "\n",
        "    articles = []\n",
        "    count = 0\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(\"Scraping:\", url)  # Print the URL being scraped\n",
        "        print(\"Status code:\", response.status_code)  # Print the status code\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        results = soup.find_all(\"a\", {\"data-selenium-selector\": \"title-link\"})\n",
        "\n",
        "        for result in results:\n",
        "            if len(articles) >= num_articles:\n",
        "                break\n",
        "\n",
        "            article_url = base_url + result[\"href\"]\n",
        "            article_response = requests.get(article_url, headers=headers)\n",
        "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
        "\n",
        "            title = article_soup.find(\"h1\", class_=\"paper-detail-header__title\").text.strip()\n",
        "            venue = article_soup.find(\"div\", class_=\"paper-meta-item\").text.strip()\n",
        "            year = article_soup.find(\"span\", class_=\"paper-meta-item__meta-venue\").text.strip().split()[-1]\n",
        "            authors = [author.text.strip() for author in article_soup.find_all(\"span\", class_=\"author-item\")]\n",
        "            abstract = article_soup.find(\"meta\", {\"name\": \"description\"})[\"content\"]\n",
        "\n",
        "            articles.append({\n",
        "                \"Title\": title,\n",
        "                \"Venue\": venue,\n",
        "                \"Year\": year,\n",
        "                \"Authors\": authors,\n",
        "                \"Abstract\": abstract\n",
        "            })\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        print(f\"Scraped {count} articles.\")\n",
        "\n",
        "        next_page = soup.find(\"a\", class_=\"next-page-link\")\n",
        "        if next_page:\n",
        "            url = base_url + next_page[\"href\"]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return articles\n",
        "\n",
        "keyword = \"XYZ\"\n",
        "num_articles = 1000\n",
        "\n",
        "articles = scrape_semantic_scholar(keyword, num_articles)\n",
        "\n",
        "if articles:\n",
        "    # Saving the collected data to a CSV file\n",
        "    import csv\n",
        "\n",
        "    keys = articles[0].keys()\n",
        "    with open('articles_semantic_scholar.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(articles)\n",
        "\n",
        "    print(\"Data saved to articles_semantic_scholar.csv\")\n",
        "else:\n",
        "    print(\"No articles were scraped. Please check if the scraping process encountered any issues.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtKskTzbCLaU",
        "outputId": "127d77f8-a1e8-4eff-a8a1-08fe465c821d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP error occurred: 400 Client Error: Bad Request for url: https://graph.instagram.com/_shashank_varma/media?fields=id%2Cmedia_type%2Cmedia_url%2Cpermalink%2Ctimestamp%2Ccaption&limit=20&access_token=your_access_token\n",
            "Response content: b\"Sorry, this content isn't available right now\"\n",
            "Failed to collect Instagram data. Check the error messages for details.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Instagram Graph API endpoint URLs\n",
        "BASE_URL = 'https://graph.instagram.com'\n",
        "ACCESS_TOKEN = 'your_access_token'\n",
        "\n",
        "# Function to make API requests to Instagram Graph API\n",
        "def make_api_request(endpoint, params):\n",
        "    url = f'{BASE_URL}/{endpoint}'\n",
        "    params['access_token'] = ACCESS_TOKEN\n",
        "    response = requests.get(url, params=params)\n",
        "    try:\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.HTTPError as e:\n",
        "        print(f'HTTP error occurred: {e}')\n",
        "        print(f'Response content: {response.content}')\n",
        "        return None\n",
        "\n",
        "# Function to collect user's media\n",
        "def collect_user_media(user_id, num_posts):\n",
        "    media_data = []\n",
        "    params = {\n",
        "        'fields': 'id,media_type,media_url,permalink,timestamp,caption',\n",
        "        'limit': num_posts\n",
        "    }\n",
        "    response_data = make_api_request(f'{user_id}/media', params)\n",
        "    if response_data and 'data' in response_data:\n",
        "        media_data.extend(response_data['data'])\n",
        "    return media_data\n",
        "\n",
        "# Specify user ID and number of posts to collect\n",
        "user_id = '_shashank_varma'  # Use 'self' for your own account or specify user ID\n",
        "num_posts = 20\n",
        "\n",
        "# Collect user's media\n",
        "media = collect_user_media(user_id, num_posts)\n",
        "\n",
        "if media:\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    df = pd.DataFrame(media)\n",
        "\n",
        "    # Save the collected data to a CSV file\n",
        "    df.to_csv('instagram_data.csv', index=False)\n",
        "\n",
        "    print(\"Data saved to instagram_data.csv\")\n",
        "else:\n",
        "    print(\"Failed to collect Instagram data. Check the error messages for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOhks1dXWEe"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqmHVEwaWhbV"
      },
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Web scrapping is a technique to  get data from social media platforms .which is a essential technique  for data science fields , research and business intelligence fields .  web scrapping has a essential aspects which are learning HTTP requests , responses , handling pagination and implementing error handling mechanisms . most challenging while  web scrapping  is dealing dynamic content which is generated by javascript  , as traditional web scrapping  libraries ."
      ],
      "metadata": {
        "id": "ScpCViFLIsWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}